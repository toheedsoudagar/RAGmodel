# ingest.py
from pathlib import Path
import os
from collections import defaultdict
import textwrap
import logging

# LangChain / Chroma imports
from langchain_community.document_loaders import (
    PyPDFLoader,
    TextLoader,
    CSVLoader,
    UnstructuredExcelLoader
)
from langchain_community.vectorstores.utils import filter_complex_metadata 
from langchain_unstructured.document_loaders import UnstructuredLoader 
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings 
from langchain_chroma import Chroma
from langchain_core.documents import Document
from tempfile import TemporaryDirectory

# OCR Handling
try:
    from pdf2image import convert_from_path
    import pytesseract
    OCR_AVAILABLE = True
except Exception:
    OCR_AVAILABLE = False

# --------- Configuration ----------
DOCS_DIR = "docs"
DB_DIR = "chroma_db"
EMBEDDING_MODEL = "embeddinggemma:latest"
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 400
# ----------------------------------

def ocr_pdf_to_documents(pdf_path):
    """Convert PDF pages to text via OCR and return a list of LangChain Documents."""
    docs = []
    if not OCR_AVAILABLE:
        print(f"[ingest][ocr] OCR dependencies not available (pdf2image/pytesseract). Skipping OCR for {pdf_path}.")
        return docs

    print(f"[ingest][ocr] Running OCR fallback on {pdf_path} ...")
    try:
        with TemporaryDirectory() as tmpdir:
            images = convert_from_path(pdf_path, dpi=200, output_folder=tmpdir)
            for i, img in enumerate(images):
                try:
                    text = pytesseract.image_to_string(img)
                except Exception as e:
                    print(f"[ingest][ocr] pytesseract failed on page {i}: {e}")
                    text = ""
                if text and text.strip():
                    docs.append(Document(page_content=text, metadata={"source": Path(pdf_path).name}))
    except Exception as e:
        print(f"[ingest][ocr] Failed to OCR {pdf_path}: {e}")
    print(f"[ingest][ocr] OCR produced {len(docs)} page documents for {pdf_path}")
    return docs

def load_all_documents():
    docs = []
    path = Path(DOCS_DIR)

    if not path.exists():
        raise ValueError(f"Docs directory {DOCS_DIR} does not exist. Create it and add documents.")

    for file in sorted(path.iterdir()):
        try:
            suffix = file.suffix.lower()

            # Skip .sql files (handled by db_setup.py)
            if suffix == ".sql":
                print(f"[ingest] Skipping SQL schema file: {file}")
                continue

            if suffix == ".pdf":
                print(f"[ingest] Loading PDF → {file}")
                try:
                    pdf_docs = PyPDFLoader(str(file)).load()
                    non_empty_pages = sum(1 for d in pdf_docs if (d.page_content or "").strip())
                    # If PDF is mostly empty images, try OCR
                    if non_empty_pages < max(1, len(pdf_docs) // 2):
                        print("[ingest] PDF text extraction is sparse — attempting OCR fallback.")
                        ocr_docs = ocr_pdf_to_documents(str(file))
                        if ocr_docs:
                            pdf_docs = ocr_docs
                    for d in pdf_docs:
                        d.metadata["source"] = file.name
                    docs.extend(pdf_docs)
                except Exception as e:
                    print(f"[ingest] Standard PDF load failed, trying OCR directly: {e}")
                    ocr_docs = ocr_pdf_to_documents(str(file))
                    docs.extend(ocr_docs)

            elif suffix == ".txt":
                print(f"[ingest] Loading TXT → {file}")
                txt_docs = TextLoader(str(file), encoding='utf-8', autodetect_encoding=True).load()
                for d in txt_docs:
                    d.metadata["source"] = file.name
                docs.extend(txt_docs)

            elif suffix == ".csv":
                print(f"[ingest] Loading CSV → {file}")
                csv_docs = CSVLoader(str(file)).load()
                for d in csv_docs:
                    d.metadata["source"] = file.name
                docs.extend(csv_docs)

            elif suffix in [".xls", ".xlsx"]:
                print(f"[ingest] Loading XLSX/XLS (Unstructured) → {file}")
                excel_docs = UnstructuredExcelLoader(str(file)).load()
                for d in excel_docs:
                    d.metadata["source"] = file.name
                docs.extend(excel_docs)

            elif suffix in [".doc", ".docx", ".ppt", ".pptx", ".html", ".htm"]:
                print(f"[ingest] Loading {suffix.upper()} (Unstructured) → {file}")
                other_docs = UnstructuredLoader(str(file)).load()
                for d in other_docs:
                    d.metadata["source"] = file.name
                docs.extend(other_docs)

        except Exception as e:
            print(f"[ingest] Failed to load {file}: {e}")

    if len(docs) == 0:
        print(f"[ingest] WARNING: No documents found or extracted in ./{DOCS_DIR} folder!")
        
    print(f"[ingest] Loaded {len(docs)} raw document sections.")
    return docs

def split_into_chunks(docs):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP
    )
    chunks = splitter.split_documents(docs)
    print(f"[ingest] Created {len(chunks)} chunks.")
    return chunks

def build_vectorstore(chunks):
    if not chunks:
        print("[ingest] No chunks to ingest.")
        return None
        
    print(f"[ingest] Building embeddings using model: {EMBEDDING_MODEL}")
    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)

    vectordb = Chroma.from_documents(
        chunks,
        embeddings,
        persist_directory=DB_DIR
    )
    print("[ingest] Chroma DB built and persisted.")
    return vectordb

def run_ingestion_if_needed():
    # Simple check: if DB dir is empty or doesn't exist
    if not os.path.exists(DB_DIR) or not os.listdir(DB_DIR):
        print("[ingest] No DB found → Running ingestion...")
        docs = load_all_documents()
        if docs:
            chunks = split_into_chunks(docs)
            print("[ingest] Filtering complex metadata for Chroma compatibility...")
            chunks = filter_complex_metadata(chunks)
            build_vectorstore(chunks)
            print("[ingest] Ingestion complete.")
    else:
        print("[ingest] DB already exists → Skipping ingestion.")

if __name__ == "__main__":
    run_ingestion_if_needed()